\documentclass[10pt,twocolumn,letterpaper]{article}
%% Welcome to Overleaf!
%% If this is your first time using LaTeX, it might be worth going through this brief presentation:
%% https://www.overleaf.com/latex/learn/free-online-introduction-to-latex-part-1

%% Researchers have been using LaTeX for decades to typeset their papers, producing beautiful, crisp documents in the process. By learning LaTeX, you are effectively following in their footsteps, and learning a highly valuable skill!

%% The \usepackage commands below can be thought of as analogous to importing libraries into Python, for instance. We've pre-formatted this for you, so you can skip right ahead to the title below.

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{multirow}
%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{natbib}
\bibliographystyle{unsrt}
%% Title
\title{
		\usefont{OT1}{bch}{b}{n}
		\normalfont \normalsize \textsc{NLP course 2021 - University of Bologna} \\ [10pt]
		\huge Assignment 2 Report \\
}
\selectlanguage{english}
\usepackage{authblk}
\author{Michele Luca Contalbo, Ehtsham Akhter, Vida Zahedi, Yiran Zeng}

\begin{document}
\maketitle

\begin{abstract}
This paper is a report which describes the second assignment given at the Natural Language Processing course at the University of Bologna. The assignment is centered on a small portion of the \textbf{fact checking} problem, which aims to estabilish whether a given statement is supported or refuted by a set of evidences. We were asked, given the \href{https://fever.ai/}{FEVER} dataset, to create some neural network models able to face this problem and compare their results. The tested models differ on how the sentence embeddings are extracted and combined together. Based on the metrics used (accuracy, f1-score, recall and precision), among all the architectures tested, the RNNs models provide better results.
\end{abstract} \\ 
\\ 
{\textbf{Keywords} \\
fact checking, FEVER, RNN, sentence embedding}

\section{Introduction}
In this section we will provide information about the assignment, the dataset structure and its preprocessing.\\
In this assignment we need to check if some claims are supported or refuted by set of evidences. The dataset used is \href{https://fever.ai/}{FEVER}, which as been devised with the idea to aid creating software able to tackle false information coming from unreliable sources. 

\subsection{Dataset Structure and Preprocessing}
The dataset is about facts taken from Wikipedia documents and it consists of 185,445 claims manually verified and classified as \texttt{Supported}, \texttt{Refuted} or \texttt{NotEnoughInfo}.
The dataset is composed by the following features:
\begin{itemize}
\item \texttt{\textbf{ID}}: id associated to the fact to verify;
\item \texttt{\textbf{Verifiable}}: whether the claim is verifiable or not;
\item \texttt{\textbf{Claim}}: fact to be verified;
\item \texttt{\textbf{Evidence}}: data structure composed by IDs that can be associated to the claim;
\item \texttt{\textbf{Label}}: whether the \texttt{\textbf{Evidence}} \texttt{Supports}, \texttt{Refutes} or has \texttt{NotEnoughInfo} on the claim.
\end{itemize}

For this assignment, we are not interested in non verifiable claims, hence some preprocessing is done in order to filter out data which is not useful for our problem. In particular, we will not consider the \texttt{Verifiable} feature and focus only on \texttt{Evidence} supporting or refuting its corresponding \texttt{Claim}. Also, the \texttt{Evidence} feature has been modified in order to contain text and not IDs.\\
\subsection{Text Preprocessing}
Standard text preprocessing strategies have been applied with the aim to reduce noise and focus on more semantically meaningful words. Each word in all sentences, after having created the vocabulary, have all been converted to their respective vocabulary IDs. The sentences have been padded to 122, which is the maximum sentence length between \texttt{Claim} and \texttt{Evidence} in training, validation and testing set.

\section{Architectures and Evaluation Methods}

\subsection{General Structure}
Each model differs from each other on how sentence embedding are extracted and combined together. Each architecture has the following structure:\\

\begin{tabular}{ |p{2cm}|p{3cm}|  }
\hline
\multicolumn{2}{|c|}{\textbf{Architecture General Structure}} \\
\hline
\textbf{Layer name}& \textbf{Input $\rightarrow$ Output size} \\
\hline
Embedding & 122 $\rightarrow$ (122,50) \\\hline
Sentence Embedding & ---- \\\hline
Merging & ---- \\\hline
Dense & 50 or 100 (concatenation) $\rightarrow$ 2 \\
\hline
\end{tabular}\vspace{0.5cm}
\\

The Embedding Layer is fed with 2 inputs of size 122. The embedding matrix, used in the first layer, has been obtained from \textbf{GloVe}\footnotemark{} with \textbf{embedding size} of $50$. In the last one, \texttt{softmax} as been used as activation. In each layer, \textbf{masking} has been applied to avoid considering padded values. The models have been trained on $5$ \textbf{epochs} and \textbf{batch size} of $64$, with \textbf{Adam} as optimizer and \textbf{cross entropy} as loss function.

\subsection{Sentence Embedding}

The models implemented to obtain sentence embeddings are the following:
\begin{itemize}
\item RNN layer with last output as sentence embedding;
\item RNN layer with mean of all outputs as sentence embedding;
\item MLP layer with reshaping;
\item Mean of token embeddings (\textbf{bag of vectors})
\end{itemize}

The following table summarizes each of their structure:\\

\begin{tabular}{ |p{1.9cm}|p{1.9cm}|p{1.9cm}|  }
\hline
\multicolumn{3}{|c|}{\textbf{Sentence Embedding Structure}} \\
\hline
\textbf{Model}& \textbf{Layers} & \textbf{Input $\rightarrow$ Output size}\\
\hline
RNN with last output & RNN & (122,50) $\rightarrow$ 50 \\\hline
%\multirow{2}{*}{Multirow}&X&Y\\&X&Y\\
\multirow{2}{*}{\shortstack[1]{RNN with \\output mean}} & RNN & (122,50) $\rightarrow$ (122,50)\\\cline{2-3} & Average Sentence\footnotemark{} & (122,50) $\rightarrow$ 50 \\\hline
\multirow{4}{*}{MLP} & Reshape & (122,50) $\rightarrow$ 6100 \\\cline{2-3}
& Dense & 6100 $\rightarrow$ 256\\\cline{2-3}
& Dense & 256 $\rightarrow$ 128\\\cline{2-3}
& Dense & 128 $\rightarrow$ 50\\\hline
Bag of vectors & Average Sentence & (122,50) $\rightarrow$ 50 \\
\hline
\end{tabular}\vspace{0.5cm}
\\

\footnotetext{\href{https://nlp.stanford.edu/projects/glove/}{https://nlp.stanford.edu/projects/glove/}}
\footnotetext{Lambda Layer}

\subsection{Merging}

After having embedded the \texttt{Claim} and \texttt{Evidence} sentences, we must decide how to merge the results. The following strategies have been tested:
\begin{itemize}
\item \textbf{Concatenation}: concatenation of claim and evidence sentence embeddings (size doubled);
\item \textbf{Sum}: sum of claim and evidence sentence embeddings;
\item \textbf{Average}: average of claim and evidence sentence embeddings.

\end{itemize}

\section{Results}

The results section is probably next easiest to write after the Methods section, since it essentially boils down to presenting your data. If anything, the production of good, high quality figures is the most important and potentially time-consuming part of this. However, make sure to not analyze any of your results here! All of that belongs in the discussion.

Including figures into \LaTeX\ can seem intimidating at first, but Overleaf makes it easy: simply click the 'Project' button above, select 'Files', and upload away from your computer. Then, insert the file name into the appropriate section of the code below.  Figure 1  shows the output of such code. A pretty good guide to formatting figures can be found at \url{https://en.wikibooks.org/wiki/LaTeX/Floats,_Figures_and_Captions#Figures}.
\\

{\scriptsize
\begin{verbatim}
\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{test.png}
    \caption{Hello!}
\end{figure}
\end{verbatim}
}



\section{Discussion}
And here is the 'meat' of the paper, so to speak. This is where you interpret your results, pointing out interesting trends within your data and how they relate to your initial hypothesis. This is also the place to justify your methodology, if you're so inclined (i.e. Why did you specifically use a certain statistical test over another? Why this tool over that tool?). Lastly, you're going to want to discuss potential sources of error. Make sure to make explicit reference to figures/tables when discussing your data; it can be helpful to walk the reader through your own personal interpretation of each figure in order. Although we recommend looking at past winning papers over at the STEM Fellowship Journal's website anyways, referring to those papers might prove most helpful when it comes to writing your discussion.

\section*{Conclusions}
What are the long-term implications of your findings? Wrap up your discussion succinctly while pointing out the significance of your work as well as it what it means for the fields you examined as much as possible. Lastly, suggest ideas for future studies that could build on your work, and justify why they might be useful. Otherwise, you're all done!

\section*{Acknowledgements}
Anyone to thank/credit for helping your team along the way? This is the place to do it!

\bibliography{bibliography}
\end{document}
